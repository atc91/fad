---
title: "Proyecto Final Fundamentos de Analítica"
output: html_notebook
---

# Problema

Usted es el encargado de analítica de una empresa de telefonía celular y tiene que proporcionar soluciones para hacer frente a las problemáticas de un sector que ha llegado a saturación del mercado. Tanto su empresa como sus competidores directos tienen que disputarse por una base de clientes limitada, de tal forma que usted tiene que responder a un objetivo estratégico definido por la dirección así: 

“Mantener y fidelizar a nuestros clientes por medio de un servicio de calidad que se adapte a sus necesidades particulares.”

Su compañía dispone de una base de datos histórica de personas que hace un año eran clientes propios. Algunos de esos clientes siguen siéndolo hoy en día, otros ya no lo son.

Se han identificado dos proyectos de analítica de datos que permitirán alcanzar tal objetivo, que tendrá que desarrollar en las dos partes siguientes. 


## Parte 1

Cree un modelo predictivo de deserción que permita identificar los clientes propensos a irse a las empresas en directa competencia con la suya en el próximo año.

### 1. ¿Encuentra alguna anomalía en los datos? (0.3)



```{r}
library(ggplot2)
library(caret)
library(corrplot)
library(fpc)
library(cluster)
library(data.table)
library(reshape2)
#library(tidyverse)  # data manipulation
library(factoextra) # clustering algorithms & visualization
```

```{r}
clientes <- data.frame(read.csv("PF-02-DatosTelco.csv", header = TRUE))
head(clientes)
```

Vamos a analizar entonces las variables numéricas para evaluar que tipo de preprocesamientos debemos hacer para no influenciar de manera inadecuada los análisis que realizaremos.
```{r}
str(clientes)
summary(clientes)
apply(clientes[, -c(1)], 2, mean)

apply(clientes[, -c(1)], 2, var)
```

Podemos ver que las variables tienen promedios muy diferentes, que impedirán a los algoritmos de clustering (basados en distancia)
encontrar estructuras adecuadas. Además, podemos ver también que las varianzas son muy diferentes, lo que influenciaría negativamente la búsqueda de los componente principales de PCA, que se basa en la dispersión de los datos. Vamos entonces a estandarizar los datos para arreglar estos problemas.

```{r}
modelo_std<- preProcess(clientes, method = c("center", "scale"))
clientes_std <- predict(modelo_std, clientes)
head(clientes_std)
```


```{r}
ggplot(stack(clientes_std), aes(x = ind, y = values)) + geom_boxplot(aes(fill=ind))

```
Podemos ver que en las variables CASA, PRECIO_DISPOSITIVO y MESES presentan valores atipicios. Pero estos valores atipicos pueden estar fuertemente correlacionados con la posibilidad de que sea un cliente satisfecho con el servicio, pues el consumira el mejor paquete de servicio.

### 2. La empresa considera que los valores de variables que estén a más de 4 desviaciones estándar del promedio deberían ser consideradas excepcionales, y por lo tanto no se deben considerar en los análisis. Identifíquelas y apártelas del dataset (0.3)


Como todos los datos estan estandarizados, su varianza es 1:

```{r}
apply(clientes_std[, -c(1)], 2, var)
```

Calculamos la desviacion de los datos estandarizados:
```{r}
means = apply(clientes_std[, -c(1)], 2, mean)
means
```

Para encontrar los outliers basta con saber cuales superan de maginutud 4 veces la media de cada variable, esto es posible porque como los datos normalizados tienen un varianza de 1, por tanto la media +- desv*4 => media +- raiz(1)*4 => media +- 4. 

```{r}
indicesAQuitarNuevos <- which( abs(clientes_std$PRECIO_DISPOSITIVO) > means["PRECIO_DISPOSITIVO"] + 4)
clientes_std[indicesAQuitarNuevos,]
```

Puede que un cliente sea excepcional en varias variables, por lo que no queremos eliminar de nuestro análisis más clientes de lo necesario. Vamos a analizar entonces variable por variable los clientes excepcionales y a guardar sus índices en el dataframe progresivamente, para poder eliminarlos al final.


```{r}
indicesAQuitar <- NULL 
indicesAQuitar <- union(indicesAQuitar, indicesAQuitarNuevos)

indicesAQuitarNuevos <- which( abs(clientes_std$INGRESOS) > means["INGRESOS"] + 4)
indicesAQuitar <- union(indicesAQuitar, indicesAQuitarNuevos)


indicesAQuitarNuevos <- which( abs(clientes_std$CASA) > means["CASA"] + 4)
indicesAQuitar <- union(indicesAQuitar, indicesAQuitarNuevos)

indicesAQuitarNuevos <- which( abs(clientes_std$MESES)> means["MESES"] + 4)

indicesAQuitar <- union(indicesAQuitar, indicesAQuitarNuevos)
indicesAQuitarNuevos <- which( abs(clientes_std$DURACION) > means["DURACION"] + 4)

indicesAQuitar <- union(indicesAQuitar, indicesAQuitarNuevos)
indicesAQuitarNuevos <- which( abs(clientes_std$SOBRECARGO) > means["SOBRECARGO"] + 4)

indicesAQuitar <- union(indicesAQuitar, indicesAQuitarNuevos)
indicesAQuitarNuevos <- which( abs(clientes_std$SALDO_RESTANTE) > means["SALDO_RESTANTE"] + 4)

indicesAQuitar <- union(indicesAQuitar, indicesAQuitarNuevos)
indicesAQuitarNuevos <- which( abs(clientes_std$SATISFACCION) > means["SATISFACCION"] + 4)

indicesAQuitar <- union(indicesAQuitar, indicesAQuitarNuevos)

length(indicesAQuitar)
```

Hay en total 4 clientes que serán ignorados por el análisis, todos atipicios en la variable PRECIO_DISPOSITIVO:
```{r}
clientes_std = clientes_std[-indicesAQuitar,]
```


### 3. Analice la correlación entre las variables y explique lo que puede implicar desde el punto de vista de PCA. (0.2)


```{r}
corMat <- cor(clientes_std[, -c(1)])
corrplot(corMat, type="upper", order="hclust")

```

Se ve una fuere correlacion entre SOBRECARGO-SATISFACCION, INGRESOS-SALDO_RESTANTE
Una correlacion moderada entre CASA-INGRESOS-SALDO_RESTANTE
Una correlacion debile entre INGRESOS-DURACION

Esto puede implicar que con solo 2 o 3(de 7)
componentes principales se pueda tene un modelo con poca perdida de información.

### 4. Debe entrenar 3 tipos de modelos predictivos de diferentes familias:

#### a. Defina el protocolo de evaluación que va a utilizar para calibrar los modelos y estimar la calidad del modelo final. (0.3)

Vamos a particionar aleatoriamente "a mano" el dataset, sacando una muestra aleatoria del 75% de los datos (17369 registros) para el entrenamiento y el resto para el test con la función createDataPartition de Caret. Inicializamos el generador aleatorio para poder garantizar reproducibilidad.
```{r}
set.seed(3456) 
trainIndex <- createDataPartition(clientes[-indicesAQuitar,]$ESTADO, p = .75, list = FALSE, times = 1)

train <- clientes[ trainIndex,]
test <-  clientes[-trainIndex,]
```


El protocolo de evaluacion del modelo va a ser repeated K-Fold cross-validation

```{r}
cctrl <- trainControl(method="repeatedcv", 
                      repeats=5, 
                      number=5, 
                      summaryFunction = twoClassSummary,
                      classProbs = TRUE )
```

#### b. Establezca las métricas que va a utilizar, justificando su escogencia (0.2)

Queremos identificar los clientes mas propensos a irse a la competencia directa de la empresa, por tanto es mas costoso decir que el cliente permanece VINCULADO cuando en realidad se RETIRA que decir que el cliente se RETIRA cuando permanece VINCULADO. 

Entonces queremos optimizar el modelo para que identifique lo mejor posible los verdaderos negativos(RETIRA) y la metrica que vamos a usar es la specificity especificidad.

#### c. Calibre 3 tipos de modelos diferentes: K-NN, árbol de decisión y algún otro que propongan, utilizando las métricas y protocolo definido (1.0)

```{r}
library(MASS)
```


Modelo KNN
```{r}
model_knn <- train(ESTADO ~., data = train,
                     method = "knn", 
                     trControl = cctrl,
                     preProcess=c("center", "scale"),
                     tuneGrid=expand.grid(k=c(15, 25, 35, 45, 55, 65, 75, 85)),
                     metric="Spec")
model_knn
```
Modelo 2
```{r}
model_rpart <- train(ESTADO ~., data = train,
                     method = "rpart", 
                     trControl = cctrl,
                     preProcess=c("center", "scale"),
                     metric="Spec")
model_rpart
```

Modelo regresión logística

```{r}
model_glm <- train(ESTADO ~., data = train,
                   method = "glmStepAIC",
                   trControl = cctrl,
                   preProcess=c("center", "scale"),
                   metric="Spec")
model_glm
```
#### d. Evalúe los 3 modelos encontrados, escoja el mejor, explicando y concluyendo lo que encontró, utilizando las métricas y protocolo definido (0.5)

Modelo KNN
```{r}
predictions_knn<-predict(object=model_knn, test)
confusionMatrix(predictions_knn, test$ESTADO)
```

Modelo arbol de desicion
```{r}
predictions_rpart<-predict(object=model_rpart, test)
confusionMatrix(predictions_rpart, test$ESTADO)
``` 

Modelo regresion logistica
```{r}
predictions_glm<-predict(object=model_glm, test)
confusionMatrix(predictions_glm, test$ESTADO)
```

Los 3 modelos tienen resultados similares tanto en especifidad como en Accuracy y Kappa.
Por velocidad en entraneamiento, evaluacion, recalibracion automatica escogemos el modelo de regresion logistica como el mas adecuado.


## Parte 2

Analice los clientes que se han ido, creando un modelo de segmentación de los clientes que desertan la compañía, teniendo en cuenta sus datos socio-demográficos y comportamientos de consumo del servicio de telefonía. Interpretar el perfil de clientes asignado a cada segmento, caracterizándolos de tal manera que le permita sugerir 3 a 5 campañas de fidelización.

### 1. Definicion del numero de campañas a realizar (0.6)

((Este numero tenemos que validarlo con el cluster que den los mecanismos de clustering))

### 2. Extraiga los componentes principales, analice sus niveles de varianza explicada, e interprete los 3 más importantes en función de las variables originales. (0.6)

Para extraer los componentes principales vamos a utilizar los datos normalizados, porque la tecnica PCA se ve afectada por la distancia. Ademas vamos a quitar la columna que indica si se encuentra vinculado o no. Por lo tanto el resultado de extraer los componentes principales es:

```{r}
clientes_std$ESTADO <- NULL # Eliminamos la columna que indica si se encuentra vinculado o no
pcomp <- prcomp(clientes_std, scale=TRUE)
```

Ahora vamoa analizar la varianza de los componenetes principales que entrego el analisis.
```{r}
varianzasPC <- pcomp$sdev^2
varianzasPC
totalPC <- sum(varianzasPC)
porcentajeInfoPC <- varianzasPC / sum(varianzasPC)
porcentajeInfoPC
print("cantidad de componentes principales")
totalPC
```
Como podemos observar salen 8 componentes principales que a su vez representan la informacion de la siguiente forma:

```{r}
dfPorcentajes = data.frame(PC=1:8, simple=porcentajeInfoPC, acumulado=cumsum(porcentajeInfoPC))
dfPorcentajes
```

Como podemos ver solo hasta el quinto componente principal podemos decir que la informacion se representa en un 87.5%, lo cual indica que las variables no estan muy correlacionadas entre si. 

((( FALTA HACER LA RELACION CON LAS VARIABLES )))
```{r}

```

### 3. Compare de los clusters obtenidos utilizando K-Means y Clustering jerárquico, seleccionando los resultados de uno de los dos métodos. Justifique. (0.4)

### K-Means

Este es el proceso para obtener los cluster con K-Means

```{r}
k <- 3
set.seed(1234)
kmClustering3 <- kmeans(clientes_std, k, nstart=100, iter.max=150)
```


```{r}
fviz_cluster(kmClustering3, data = clientes_std)
```